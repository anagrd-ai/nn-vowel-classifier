{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1nY-KduBPR3hUK3aGosaa7WnYRwrs4Av-","authorship_tag":"ABX9TyMTtDHcnJdwfi+S29DC5p3v"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T_F8qyTe6CvE","executionInfo":{"status":"ok","timestamp":1719360467389,"user_tz":420,"elapsed":11170,"user":{"displayName":"Jason Smart","userId":"11984969484087358904"}},"outputId":"b844e397-837d-4138-faa6-baed2b0d2652"},"outputs":[{"output_type":"stream","name":"stdout","text":["Input text size:  20234\n","Epoch: 1, Loss: 0.4262\n","Epoch: 2, Loss: 0.1727\n","Epoch: 3, Loss: 0.1027\n","Epoch: 4, Loss: 0.0703\n","Epoch: 5, Loss: 0.0521\n","Epoch: 6, Loss: 0.0406\n","Epoch: 7, Loss: 0.0328\n","Epoch: 8, Loss: 0.0272\n","Epoch: 9, Loss: 0.0231\n","Epoch: 10, Loss: 0.0200\n","evaluate model, labels:  [0, 1, 1, 0, 0, 1, 0, 0, 0, 0]  preds  [0, 1, 1, 0, 0, 1, 0, 0, 0, 0]\n","Test Accuracy: 1.0000\n","              precision    recall  f1-score   support\n","\n","           0       1.00      1.00      1.00      3074\n","           1       1.00      1.00      1.00       973\n","\n","    accuracy                           1.00      4047\n","   macro avg       1.00      1.00      1.00      4047\n","weighted avg       1.00      1.00      1.00      4047\n","\n","Text: apple  Label: 1\n","Text: pear  Label: 0\n","Text: cinnamon  Label: 0\n","Text: every  Label: 1\n","Text: underhill  Label: 1\n","Text: is  Label: 1\n","Text: the  Label: 0\n","Text: star  Label: 0\n","Text: a  Label: 1\n","Text: A  Label: 1\n","Text: of  Label: 1\n","Text: From  Label: 0\n","Text: Quantum  Label: 0\n","Text: Marginal  Label: 0\n","Text: Await  Label: 1\n","Text: awash  Label: 1\n","Text: serene  Label: 0\n","Text: mash  Label: 0\n","Text: young  Label: 0\n","Text: Untoward  Label: 1\n","Text: untoward  Label: 1\n"]}],"source":["'''\n","Supervised Learning model that identifies words that start with a vowel.\n","That's it.\n","'''\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.metrics import accuracy_score, classification_report\n","from torchtext.vocab import build_vocab_from_iterator\n","from torchtext.data.utils import get_tokenizer\n","\n","# Load the dataset, will be split into train/test\n","# Comma separated words and 1 or 0 to denote starts w vowel or not, eg \"apple,1\"\n","\n","training_file = \"/content/drive/MyDrive/Colab Notebooks/ClassifyVowels/starts_with_vowel_all.txt\"\n","df = pd.read_csv(training_file, dtype=str, keep_default_na=False)\n","df_texts = df['text'].values\n","df_labels = df['label'].values\n","\n","print(\"Input text size: \", len(df_texts))\n","\n","# Encode the labels\n","label_encoder = LabelEncoder()\n","encoded_labels = label_encoder.fit_transform(df_labels)\n","\n","# Split the data into training and test sets\n","texts_train, texts_test, labels_train, labels_test = train_test_split(\n","    df_texts, encoded_labels, test_size=0.2,\n","    random_state=42  # specify the random seed\n",")\n","\n","ascii_vocab = {chr(i): i-32 for i in range(32, 126)}  # printable ascii\n","def get_vocab_idx(word: str) -> int:\n","  if len(word) > 0 and word[0] in ascii_vocab:\n","    return ascii_vocab[word[0]]\n","  return 0\n","\n","# Define a custom dataset\n","class FirstCharacterTextDataset(Dataset):\n","    def __init__(self, texts, labels, tokenizer):\n","        self.texts = texts\n","        self.labels = labels\n","        self.tokenizer = tokenizer\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, idx):\n","        text = self.texts[idx]\n","        label = self.labels[idx]\n","        # tokens = self.tokenizer(text)\n","        # token_ids = [self.vocab[token] for token in tokens]\n","        token_ids = [ get_vocab_idx(text) ]\n","        return torch.tensor(token_ids, dtype=torch.long), torch.tensor(label, dtype=torch.long)\n","\n","# Tokenizer and Vocabulary\n","tokenizer = get_tokenizer('basic_english')\n","# tokenizer = get_tokenizer()  # Use a basic split function as the tokenizer\n","# vocab = build_vocab_from_iterator(map(tokenizer, df_texts), specials=[\"<unk>\"])\n","# vocab.set_default_index(vocab[\"<unk>\"])\n","\n","#print(\"Vocab: \", end=\"\")\n","#for token in list(df_texts):\n","#  print(vocab[token], \" \", end=\"\")\n","\n","# Create dataset and dataloader\n","train_dataset = FirstCharacterTextDataset(texts_train, labels_train, tokenizer)\n","test_dataset = FirstCharacterTextDataset(texts_test, labels_test, tokenizer)\n","\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=lambda x: x)\n","test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=lambda x: x)\n","\n","# Define the model\n","class TextClassificationModel(nn.Module):\n","    def __init__(self, vocab_size, embed_dim, num_class):\n","        super(TextClassificationModel, self).__init__()\n","        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n","        self.fc = nn.Linear(embed_dim, num_class)\n","        self.init_weights()\n","\n","    def init_weights(self):\n","        initrange = 0.5\n","        self.embedding.weight.data.uniform_(-initrange, initrange)\n","        self.fc.weight.data.uniform_(-initrange, initrange)\n","        self.fc.bias.data.zero_()\n","\n","    def forward(self, text, offsets):\n","        embedded = self.embedding(text, offsets)\n","        return self.fc(embedded)\n","\n","# Initialize the model, loss function, and optimizer\n","num_class = len(label_encoder.classes_)\n","vocab_size = len(ascii_vocab)\n","embed_dim = 64\n","\n","model = TextClassificationModel(vocab_size, embed_dim, num_class)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(model.parameters(), lr=0.01)\n","\n","def generate_batch(batch):\n","  '''\n","  Function to generate offsets and texts.\n","\n","  :returns: the text, offsets, and labels for that batch\n","  '''\n","    label = torch.tensor([entry[1] for entry in batch])\n","    text = [entry[0] for entry in batch]\n","    offsets = [0] + [len(entry) for entry in text]\n","    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n","    text = torch.cat(text)\n","\n","    #print(\"generate_batch text \", text[0:7], \" labels \", label[0:7])\n","    return text, offsets, label\n","\n","# Training loop\n","for epoch in range(10):  # Number of epochs\n","    model.train()\n","    total_loss = 0\n","    for batch in train_loader:\n","        optimizer.zero_grad()\n","        text, offsets, cls = generate_batch(batch)\n","        output = model(text, offsets)\n","        loss = criterion(output, cls)\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","    print(f'Epoch: {epoch+1}, Loss: {total_loss/len(train_loader):.4f}')\n","\n","def evaluate_model(model, test_loader):\n","    model.eval()\n","    all_preds = []\n","    all_labels = []\n","    with torch.no_grad():\n","        for batch in test_loader:\n","            text, offsets, cls = generate_batch(batch)\n","            output = model(text, offsets)\n","            preds = torch.argmax(output, dim=1)\n","            all_preds.extend(preds.tolist())\n","            all_labels.extend(cls.tolist())\n","\n","    print(\"evaluate model, labels: \", all_labels[0:10], \" preds \", all_preds[0:10])\n","    return all_labels, all_preds\n","\n","# Evaluate on test set\n","labels_test, preds_test = evaluate_model(model, test_loader)\n","accuracy = accuracy_score(labels_test, preds_test)\n","print(f'Test Accuracy: {accuracy:.4f}')\n","print(classification_report(labels_test, preds_test, target_names=label_encoder.classes_))\n","\n","# Function to predict on new text data\n","def predict(model, text, tokenizer):\n","    model.eval()\n","    token_ids = torch.tensor([ get_vocab_idx(text) ], dtype=torch.long)\n","    # tokens = tokenizer(text)\n","    # token_ids = torch.tensor([vocab[token] for token in tokens], dtype=torch.long)\n","    # print(\"predict token vocab \", token_ids)\n","    offsets = torch.tensor([0])\n","    with torch.no_grad():\n","        output = model(token_ids, offsets)\n","        predicted_label = torch.argmax(output, dim=1).item()\n","    return label_encoder.inverse_transform([predicted_label])[0]\n","\n","# Test the prediction function\n","# sample_text = \"your sample text here\"\n","sample_texts = [\"apple\", \"pear\", \"cinnamon\", \"every\", \"underhill\", \"is\", \"the\",\n","                \"star\", \"a\", \"A\", \"of\", \"From\", \"Quantum\", \"Marginal\", \"Await\",\n","                \"awash\", \"serene\", \"mash\", \"young\", \"Untoward\", \"untoward\"]\n","for sample_text in sample_texts:\n","  predicted_label = predict(model, sample_text, tokenizer)\n","  print(f'Text: {sample_text}  Label: {predicted_label}')\n"]}]}